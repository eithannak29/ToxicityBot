{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sys.path.append('../utils/')\n",
    "sys.path.append('..')\n",
    "from preprocessing import load_preprocessed_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données prétraitées\n",
    "Ici on charge les ensembles de données d'entraînement, de validation et de test en utilisant une fonction personnalisée définie dans le module de prétraitement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 sec TO LOAD\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'data'\n",
    "df_train, df_val, df_test = load_preprocessed_dataframe(output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage des exemples de commentaires\n",
    "Ici on affiche quelques exemples de commentaires du jeu de données d'entraînement pour avoir un aperçu des données textuelles sur lesquelles nous travaillons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140030    Grandma Terri Should Burn in Trash \\nGrandma T...\n",
      "159124    , 9 May 2009 (UTC)\\nIt would be easiest if you...\n",
      "60006     \"\\n\\nThe Objectivity of this Discussion is dou...\n",
      "65432                 Shelly Shock\\nShelly Shock is. . .( )\n",
      "154979    I do not care. Refer to Ong Teng Cheong talk p...\n",
      "                                ...                        \n",
      "119879    REDIRECT Talk:John Loveday (experimental physi...\n",
      "103694    Back it up. Post the line here with the refere...\n",
      "131932    I won't stop that. Sometimes Germanic equals G...\n",
      "146867    \"\\n\\n British Bands?  \\n\\nI think you've mista...\n",
      "121958    You are WRONG. \\n\\nJustin Thompson is mentione...\n",
      "Name: comment_text_baseline, Length: 127656, dtype: object\n",
      "--------------------------\n",
      "140030    grandma terri should burn in trash grandma ter...\n",
      "159124    , may ( utc ) it would be easiest if you were ...\n",
      "60006     `` the objectivity of this discussion is doubt...\n",
      "65432                shelly shock shelly shock is . . . ( )\n",
      "154979    i do not care . refer to ong teng cheong talk ...\n",
      "                                ...                        \n",
      "119879    redirect talk : john loveday ( experimental ph...\n",
      "103694    back it up . post the line here with the refer...\n",
      "131932    i wo n't stop that . sometimes germanic equals...\n",
      "146867    `` british bands ? i think you 've mistaken sc...\n",
      "121958    you are wrong . justin thompson is mentioned i...\n",
      "Name: comment_text_word_tokenize_simple_normalization, Length: 127656, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"comment_text_baseline\"])\n",
    "print(\"--------------------------\")\n",
    "print(df_train[\"comment_text_word_tokenize_simple_normalization\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation des textes\n",
    "Ici on concatène les commentaires des ensembles de données d'entraînement, de validation et de test en une seule liste de phrases, puis affiche la longueur totale de cette liste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming df_train, df_val, df_test contain a column 'text' with the sentences.\n",
    "all_texts = pd.concat([df_train['comment_text_word_tokenize_simple_normalization'], df_val['comment_text_word_tokenize_simple_normalization']])\n",
    "sentences = all_texts.tolist()\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du modèle de bigrammes\n",
    "Ici on utilise `CountVectorizer` pour créer un modèle de bigrammes (séquences de deux mots) à partir des phrases. Le modèle est ensuite ajusté sur les phrases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul et affichage de la fréquence des bigrammes\n",
    "Ici on calcule la fréquence de chaque bigramme dans le corpus et définit une fonction pour prédire le mot suivant basé sur un mot précédent en utilisant les bigrammes les plus fréquents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count the frequency of each bigram\n",
    "bigram_frequency = np.asarray(X.sum(axis=0)).ravel()\n",
    "\n",
    "# Map each bigram to its frequency\n",
    "bigram_to_freq = dict(zip(vectorizer.get_feature_names_out(), bigram_frequency))\n",
    "\n",
    "# Sort the bigram_to_freq dictionary by frequency in descending order and print the top 10 bigrams\n",
    "#for bigram, freq in sorted(bigram_to_freq.items(), key=lambda item: item[1], reverse=True)[:10]:\n",
    "#    print(f\"Bigram: '{bigram}', Frequency: {freq}\")\n",
    "\n",
    "# Function to predict the next word\n",
    "def predict_next_word(previous_word):\n",
    "    candidates = {bigram: freq for bigram, freq in bigram_to_freq.items() if bigram.startswith(previous_word + ' ')}\n",
    "    \n",
    "    sorted_candidates = {k: v for k, v in sorted(candidates.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    \n",
    "    #for bigram, freq in sorted_candidates.items():\n",
    "        #print(f\"Bigram: '{bigram}', Frequency: {freq}\")\n",
    "\n",
    "    if not candidates:\n",
    "        return \"No prediction available\"\n",
    "    return max(candidates, key=candidates.get).split()[1]\n",
    "\n",
    "# Example\n",
    "print(predict_next_word(\"you\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du modèle de trigrammes\n",
    "Ici on utilise `CountVectorizer` pour créer un modèle de trigrammes (séquences de trois mots) à partir des phrases. Le modèle est ensuite ajusté sur les phrases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer_3gram \u001b[38;5;241m=\u001b[39m CountVectorizer(token_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m X_3gram \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer_3gram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Samy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Samy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Samy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1313\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1311\u001b[0m     indices_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mint32\n\u001b[0;32m   1312\u001b[0m j_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(j_indices, dtype\u001b[38;5;241m=\u001b[39mindices_dtype)\n\u001b[1;32m-> 1313\u001b[0m indptr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(indptr, dtype\u001b[38;5;241m=\u001b[39mindices_dtype)\n\u001b[0;32m   1314\u001b[0m values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(values, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintc)\n\u001b[0;32m   1316\u001b[0m X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(\n\u001b[0;32m   1317\u001b[0m     (values, j_indices, indptr),\n\u001b[0;32m   1318\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(indptr) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(vocabulary)),\n\u001b[0;32m   1319\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   1320\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorizer_3gram = CountVectorizer(token_pattern=r'\\b\\w+\\b', ngram_range=(3, 3))\n",
    "X_3gram = vectorizer_3gram.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul et affichage de la fréquence des trigrammes\n",
    "Ici on calcule la fréquence de chaque trigramme dans le corpus et définit une fonction pour prédire le mot suivant basé sur les deux mots précédents en utilisant les trigrammes les plus fréquents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trgram: 'upon a time', Frequency: 26\n",
      "Trgram: 'upon a non', Frequency: 3\n",
      "Trgram: 'upon a person', Frequency: 3\n",
      "Trgram: 'upon a fascinating', Frequency: 2\n",
      "Trgram: 'upon a full', Frequency: 2\n",
      "Trgram: 'upon a quick', Frequency: 2\n",
      "Trgram: 'upon a scarlet', Frequency: 2\n",
      "Trgram: 'upon a 19', Frequency: 1\n",
      "Trgram: 'upon a bit', Frequency: 1\n",
      "Trgram: 'upon a consent', Frequency: 1\n",
      "time\n"
     ]
    }
   ],
   "source": [
    "# Count the frequency of each bigram\n",
    "trigram_frequency = np.asarray(X_3gram.sum(axis=0)).ravel()\n",
    "\n",
    "# Map each bigram to its frequency\n",
    "trigram_to_freq = dict(zip(vectorizer_3gram.get_feature_names_out(), trigram_frequency))\n",
    "\n",
    "# Sort the bigram_to_frq dictionary by frequency in descending order and print the top 10 bigrams\n",
    "#for trigram, freq in sorted(trigram_to_freq.items(), key=lambda item: item[1], reverse=True)[:10]:\n",
    "    #print(f\"Bigram: '{trigram}', Frequency: {freq}\")\n",
    "\n",
    "# Function to predict the next word based on the two previous words\n",
    "def predict_next_word_trigram(previous_words):\n",
    "    previous_words = previous_words.lower()\n",
    "    candidates = {trigram: freq for trigram, freq in trigram_to_freq.items() if trigram.startswith(previous_words + ' ')}\n",
    "\n",
    "    sorted_candidates = {k: v for k, v in sorted(candidates.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # Now, print the sorted candidates\n",
    "    i = 0\n",
    "    for trigram, freq in sorted_candidates.items():\n",
    "        print(f\"Trgram: '{trigram}', Frequency: {freq}\")\n",
    "        i += 1\n",
    "        if (i == 10):\n",
    "            break\n",
    "\n",
    "    if not candidates:\n",
    "        return \"No prediction available\"\n",
    "    # Extracting the last word of the most frequent trigram following the previous_words\n",
    "    return max(candidates, key=candidates.get).split()[2]\n",
    "\n",
    "print(predict_next_word_trigram(\"upon a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complétion de phrase avec les bigrammes\n",
    "Ici on définit une fonction pour compléter une phrase en utilisant les bigrammes les plus fréquents. Elle prédit les mots suivants jusqu'à atteindre la longueur maximale spécifiée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you like playing the article is a few days\n"
     ]
    }
   ],
   "source": [
    "def complete_the_phrase(starting_phrase, max_length=10):\n",
    "    current_phrase = starting_phrase.strip()\n",
    "    words = current_phrase.split()\n",
    "    \n",
    "    # Continue until reaching the maximum length\n",
    "    for _ in range(max_length - len(words)):\n",
    "        last_word = words[-1]\n",
    "        # Find candidates that start with the last word of the current phrase\n",
    "        candidates = {bigram: freq for bigram, freq in bigram_to_freq.items() if bigram.startswith(last_word + ' ')}\n",
    "        if not candidates:\n",
    "            break  # No candidates found, stop the loop\n",
    "        # Pick the most frequent continuation (the second word in the bigram)\n",
    "        next_word = sorted(candidates.items(), key=lambda item: item[1], reverse=True)[0][0].split()[1]\n",
    "        words.append(next_word)\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Example usage:\n",
    "print(complete_the_phrase(\"Do you like playing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complétion de phrase avec les trigrammes\n",
    "Ici on définit une fonction pour compléter une phrase en utilisant les trigrammes les plus fréquents. Elle prédit les mots suivants jusqu'à atteindre la longueur maximale spécifiée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time when i was just a few days ago i m not\n"
     ]
    }
   ],
   "source": [
    "def complete_the_phrase_trigram(starting_phrase, max_length=15):\n",
    "    current_phrase = starting_phrase.strip()\n",
    "    words = current_phrase.split()\n",
    "    \n",
    "    # Ensure there are enough words for trigram prediction\n",
    "    if len(words) < 2:\n",
    "        return \"Please provide at least two words for the initial phrase.\"\n",
    "    \n",
    "    # Continue until reaching the maximum length\n",
    "    for _ in range(max_length - len(words)):\n",
    "        # Use the last two words for the trigram prediction\n",
    "        last_two_words = ' '.join(words[-2:])\n",
    "        # Find candidates that start with the last two words of the current phrase\n",
    "        candidates = {trigram: freq for trigram, freq in trigram_to_freq.items() if trigram.startswith(last_two_words + ' ')}\n",
    "        if not candidates:\n",
    "            break  # No candidates found, stop the loop\n",
    "        # Pick the most frequent continuation (the third word in the trigram)\n",
    "        next_word = sorted(candidates.items(), key=lambda item: item[1], reverse=True)[0][0].split()[2]\n",
    "        words.append(next_word)\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Example usage:\n",
    "print(complete_the_phrase_trigram(\"once upon\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe NGramPredictor\n",
    "Ici on définit une classe `NGramPredictor` pour créer un modèle de n-grammes (séquences de n mots) avec des méthodes pour ajuster le modèle sur des phrases, prédire le mot suivant et compléter une phrase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramPredictor:\n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "        self.vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', ngram_range=(n, n))\n",
    "        self.ngram_to_freq = {}\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        X_ngram = self.vectorizer.fit_transform(sentences)\n",
    "        ngram_frequency = np.asarray(X_ngram.sum(axis=0)).ravel()\n",
    "        self.ngram_to_freq = dict(zip(self.vectorizer.get_feature_names_out(), ngram_frequency))\n",
    "\n",
    "    def predict_next_word(self, previous_words):\n",
    "        previous_words = ' '.join(previous_words.split()[-(self.n-1):]).lower()\n",
    "        candidates = {ngram: freq for ngram, freq in self.ngram_to_freq.items() if ngram.startswith(previous_words + ' ')}\n",
    "        \n",
    "        if not candidates:\n",
    "            return None  # Changed to return None for easier checking\n",
    "        \n",
    "        return max(candidates, key=candidates.get).split()[-1]\n",
    "\n",
    "    def complete_the_phrase(self, starting_phrase, max_length=10):\n",
    "        current_phrase = starting_phrase.strip()\n",
    "        words = current_phrase.split()\n",
    "        \n",
    "        # Adjust for n-gram model\n",
    "        for _ in range(max_length - len(words)):\n",
    "            if len(words) < self.n - 1:\n",
    "                return \"Please provide more words for the initial phrase.\"\n",
    "            \n",
    "            # For n-grams, use the last n-1 words as context\n",
    "            previous_words = ' '.join(words[-(self.n-1):])\n",
    "            next_word = self.predict_next_word(previous_words)\n",
    "            \n",
    "            if not next_word:\n",
    "                break  # No candidates found, stop the loop\n",
    "            \n",
    "            words.append(next_word)\n",
    "        \n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def print_top_ngrams(self, top=10):\n",
    "        for ngram, freq in sorted(self.ngram_to_freq.items(), key=lambda item: item[1], reverse=True)[:top]:\n",
    "            print(f\"N-Gram: '{ngram}', Frequency: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation du NGramPredictor\n",
    "Ici on crée une instance de la classe `NGramPredictor` pour les trigrammes et ajuste le modèle sur les phrases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Example usage:\n",
    "ngram_predictor = NGramPredictor(n=4)  # For trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage des n-grammes les plus fréquents et complétion de phrase\n",
    "Ici on affiche les n-grammes les plus fréquents et utilise la méthode `complete_the_phrase` de la classe `NGramPredictor` pour compléter une phrase donnée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Gram: 'nigger nigger nigger nigger', Frequency: 3411\n",
      "N-Gram: 'on my talk page', Frequency: 3067\n",
      "N-Gram: 'be blocked from editing', Frequency: 2761\n",
      "N-Gram: 'fuck you fuck you', Frequency: 2681\n",
      "N-Gram: 'you fuck you fuck', Frequency: 2623\n",
      "N-Gram: 'i don t know', Frequency: 2621\n",
      "N-Gram: 'you will be blocked', Frequency: 2549\n",
      "N-Gram: 'if you continue to', Frequency: 2486\n",
      "N-Gram: 'if you have any', Frequency: 2483\n",
      "N-Gram: 'i don t think', Frequency: 2464\n",
      "once upon a time in a steppe far far away\n"
     ]
    }
   ],
   "source": [
    "ngram_predictor.print_top_ngrams()\n",
    "print(ngram_predictor.complete_the_phrase(\"once upon a\", max_length=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Évaluation du modèle\n",
    "Ici on définit une fonction pour évaluer la précision du modèle sur les phrases de test. Elle compare les prédictions du modèle avec les mots réels et calcule la précision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_sentences):\n",
    "    print(len(test_sentences))\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    counter = 0\n",
    "    for sentence in test_sentences:\n",
    "        counter += 1\n",
    "        words = sentence.split()\n",
    "        # Ensure the sentence has more words than n-1 to make a prediction and have a target\n",
    "        if len(words) >= model.n:\n",
    "            for i in range(model.n - 1, len(words)):\n",
    "                context = ' '.join(words[max(0, i - model.n + 1):i])\n",
    "                actual_next_word = words[i]\n",
    "                predicted_next_word = model.predict_next_word(context)\n",
    "                \n",
    "                if predicted_next_word is not None and predicted_next_word == actual_next_word:\n",
    "                    correct_predictions += 1\n",
    "                total_predictions += 1\n",
    "\n",
    "                if (predicted_next_word is None):\n",
    "                    print (\"None \")\n",
    "                else:\n",
    "                    print(\"Context: \" + context + \" - Expected: \" + words[i] + \" Predicted: \" + predicted_next_word + \" \" + str(counter))\n",
    "        if (counter >= 100):\n",
    "            break\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage des phrases que l'on va tester !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank you for understanding . i think very highly of you and would not revert without discussion .', ': dear god this site is horrible .', \"`` : : : somebody will invariably try to add religion ? really ? ? you mean , the way people have invariably kept adding `` '' religion '' '' to the samuel beckett infobox ? and why do you bother bringing up the long-dead completely non-existent `` '' influences '' '' issue ? you 're just flailing , making up crap on the fly . : : : for comparison , the only explicit acknowledgement in the entire amos oz article that he is personally jewish is in the categories ! ``\", \"`` it says it right there that it is a type . the `` '' type '' '' of institution is needed in this case because there are three levels of suny schools : -university centers and doctoral granting institutions -state colleges -community colleges . it is needed in this case to clarify that ub is a suny center . it says it even in binghamton university , university at albany , state university of new york , and stony brook university . stop trying to say it 's not because i am totally right in this case . ''\", \"`` == before adding a new product to the list , make sure it 's relevant == before adding a new product to the list , make sure it has a entry already , `` '' proving '' '' it 's relevance and giving the reader the possibility to read more about it . otherwise it could be subject to deletion . see this article 's revision history . ''\", 'this other one from', '== reason for banning throwing == this article needs a section on /why/ throwing is banned . at the moment , to a non-cricket fan , it seems kind of arbitrary .', '|blocked ] ] from editing |', '== arabs are committing genocide in iraq , but no protests in europe . == may europe also burn in hell .', 'please stop . if you continue to vandalize , as you did to homosexuality , you will be blocked from editing .', \"== energy == i have edited the introduction , because previously it said that passive transport does not use any kind of energy . this is not true . passive transport relies on the kinetic energy of the substance that is being transported . this kinetic energy is what causes it to move around and ( by random chance ) cross the membrane . the difference is that active transport actually uses the cell 's energy ( atp or electrochemical gradient ) to pump the substance across the membrane .\", '@ redslash , cut it short . if you have sources stating the rok is sovereign post them . otherwise please aknowledge wp is not the place to make or .', '. jews are not a race because you can only get it from your mother . your own mention of ethiopian jews not testing as jews proves it is not , as well as the fact that we accept converts', ': : : if ollie or others think that one list of the oldest people we know about is too long , the easy answer is to raise the cutoff age . is purely a round number and a full years shorter then the record . we can make it the top or top or everyone above - tell us what the maximum list size is and we can set a threshold .', 'professors to the manhatten project .', \"`` : not sure whether this is notable enough to be mentioned in the article , but you 're right versions later than ( ) can open files created by all previous versions , but save files only in its own format , with a suffix of `` '' .msx '' '' , where x is the version number ( , , , , , ... ) . this is intentional , of course . besides , all versions but save files in a compressed format. ``\", '', \"`` ==balance== this page has one sentence about the basic definition of the word , and a huge amount about the slang/profane uses . perhaps the former should be extended ; is there no information about female dogs available beyond their name ? this is an encyclopaedia , not a dictionary . i feel that whoever is looking this definition up is very appropiate and should be deleted from this word is used very often and is also a very `` '' mean '' '' word . i belive that is majorly true . very much so . okay so , the good meaning is a female dog . bitch ! ! ! ! ! ! ! ! ! it also stands for the name brittany fellows preceding unsigned comment added by ==etymology== the word bitch is from the old norse bikkjuna meaning `` '' female of the dog '' '' of unknown origin , grimm derives the old norse words from lapp pittja , but oed notes that `` '' the converse is equally possible . '' '' the adj . bitchy was first seen in . the verb meaning to complain in . slang bitchen `` '' good '' '' is attested to the 's == reclaiming the word bitch == the word bitch is actually only offencive in american and canadian english . in most other english variants bitch maintains it 's correct definition - female canine . people have argued that bitch is different because it is used as an insult - but so is `` '' pig '' '' , `` '' dog '' '' , `` '' cow '' '' and others . these are not considered profane , so why is bitch ? as far as i am concerned , there is nothing to `` '' reclaim '' '' as bitch simply means a female canine . it may be used as a pejorative or descriptor - that does not make it a profanity . it should definately say something about kyle vanderweilen and all his bitchin is there any particular evidence of women `` '' reclaiming '' '' the word bitch in the s ? can anyone point to articles on this , etc. ? the song is definitely interesting and belongs here , but does n't actually reclaim the word `` '' bitch '' '' any more than it reclaims the word `` '' sinner . '' '' also , i do n't really understand the last paragraph and it sucks i was going to try and clear it up , but i realized i do n't know what it means . can someone point to a source that lays out the argument about bitches , fertility and patriarchy more clearly ? : we do n't need articles at all as i there are definitely enough examples ( even outside of the s ) . missy elliot cleary and repeatedly reclaims the word , for instance , `` '' she 's a bitch '' '' . : :references are fine just the name `` '' missy elliot '' '' got me to find a quote of a rolling stone review that mentioned `` '' reclaiming '' '' the word . in fact , if some one knows more about her , it might be interesting to add a section on her work and on how it relates to women/slurs more generally within hip-hop . : : : how relevant is this reclamation ? it seems to me poison for a woman to use an epithet which still strongly connotes despised traits . i do n't see how using a word connoting `` '' querolous '' '' , `` '' spiteful '' '' , and `` '' malicious '' '' can be empowering . but i do n't know ; life 's a bitch . : : : :it stems from a reaction to the prevalence of tagging any woman who does n't adhere to a certain standard of femininity as a bitch . to use a literary example , in the novel the handmaid 's tale the narrator related the feeling that , whenever she outsmarted a man , she could almost hear him calling her a bitch in his mind , even her own husband . - : : : : : that literary reaction you recited is jealousy ; men have names for other men who outsmart them too , but , not having a word like `` '' bitch '' '' for them , they have to resort to a greater variety of pejoritive epithets . though you have n't been specific , i think that standard of femininity you referred to is a pretext for subordination , which both sexes often strive to impose on the other , but men have traditionally had more power . because it is such a popular epithet in slang , the connotations of `` '' bitch '' '' are diffuse , though still commonly pernicious . i think bitch , in itself , is not chiefly a denunciation to punish women who do not conform to that standard , but rather a contemptuous word often used for it . : : : : : however , i still do n't see the point of reclamati\", 'redirect talk : mi vida eres t', \"`` i 'm not convinced that he was blind . where is this documented ? it 's possible that he was just what we 'd call `` '' legally blind '' '' ie did n't have great vision , and that the name `` '' blind blake '' '' is an exaggerated moniker . although i have no proof i 've got a feeling that him being legally blind is more likely than totally blind . of course i 've got no evidence to back that thing up . ''\", \"`` == ref : ss ponzi scheme == hi padillah , it is not my opinion that i am trying to impose here , although it may appear to be so . i am just highlighting the fact that it remains controversial whether it is or is n't a ponzi scheme ( even if a legal one ) , so you can not state as a fact that it is not a ponzi scheme ( i saw the reference , and the perpetrator itself can not be treated as a `` '' reliable source '' '' ) . in fact , claiming it is not a ponzi scheme seems to be an opinion in itself . my point is that a claim should not be made either way , and the edit in question just accomplishes that . thank you , virat ''\", '== september th truce == according to several news sources , a truce was reached in minsk last night .', \"i 'd never think i 'd need to say it , but is n't a fansite discussion board . if anything is unannounced by any authority , it might as well be false . mmorpgs are overrated ,\", 'but this is not the article about government position but about the reaction . add positions to kosovo declaration of independence or foreign relations of kosovo .', 'dj robinson is gay as hell ! he sucks his dick so much ! ! ! ! !', \"== dracula 's grandmother == dracula 's grandmother was a bulgarian princess , the sister of ivan sratzimir . the links with the lands across the danube remain largely unexamined . i would appreciate any serious contributions . ( kaloyan )\", \"* i agree with billfruge . the author describes the unverifiable nature of the two competing etymology theories sufficiently well so as not to mislead any reader , renders an opinion as to which of the two is more likely ( and it is clear that this is the author 's opinion ) , and presents the reader with the opportunity to accept either or neither of the two .\", 'i will burn you to hell if you revoke my talk page access ! ! ! ! ! ! ! ! ! ! ! ! !', \"== can you work your magic ? == hi . i was wondring if you had time to help out with : featured article candidates/w . e. b. du bois/archive . any input would be appreciated : a quick scan or a full review . i 've noticed your work before and i respect your skills . thanks .\", ': fuck off , you anti-semitic cunt . |', 'puwersa ng masa !', \"`` : there are now four references , including britannica , giving the correct date , which is getting silly . you complain in your edit summary that you ca n't find them , so i 've added quotes to two , to help you out . after that you 're on your own : wp : sourceaccess has more on this . i 'm happy to concede that britannica , alone , mentions the date you offer ( but without explaining the context ) , so that 's one of the sources i 've expanded with a quote specifically mentioning august . : russian has : `` '' - `` '' ( `` '' august forces of the th tank corps , army [ of ] paulus , reached the volga north of stalingrad '' '' ) , cited from the war diary of colonel-general franz haider . : ``\", \"redirect talk : kemp 's thicket rat\", \"`` : :use of `` '' assisted suicide '' '' to refer to aid in dying is inaccurate in fourth distinctive ways . first , it was coined by catholic activists disputing oregon 's measure in - so it carries a political bias . second , the laws in oregon , washington and vermont each do not contain the term `` '' assisted suicide '' '' but use aid in dying . third , assisting a suicide remains illegal in all three states - and also montana and new mexico . no state attorney general or county district attorney has charged any physician or pharmacist who acted within the aid in dying laws of the three states . finally , describing people who use aid in dying , such as brittany maynard , suicides grossly misrepresents their experience and devalues both their lives and deaths . continued usage of the biased and inaccurate term should be amended throughout ``\", 'see also . . .', \"may please stop . if you continue to delete or edit legitimate talk page comments , as you did at talk : deepwater horizon oil spill , you may be blocked for vandalism . : if this is a shared ip address , and you did n't make the edit , consider creating an account for yourself so you can avoid further irrelevant notices .\", '== brandonyusoftoropov has requested mediation over a conflict with you == ed , [ user : brandonyusoftoropov|brandonyusoftoropov ] has requested mediation over a conflict with you . i am willing to mediate the dispute . do you accept ? if so , please end me an email at .', \"`` : that is ridiculous . unless there 's a good and non-disingenuous response , i would absolutely agree with you blocking indef outright . falsifying sources should simply never be tolerated . // ``\", \"how dare you vandalize that page about the hms beagle ! do n't vandalize again , demon !\", 'aapn bhtla aanand jhala ..', ': :no , he is an arrogant , self serving , immature idiot . get it right .', \"`` you wrote : you will need to cite reliable sources for the assertions `` '' far too large to be chemical '' '' and `` '' no endothermic storage events '' '' . i do not think any source can conclusively estabish either point . . . it is very easy to establish both points , and there are hundreds of papers with proof of them . . the limits of chemical storage ( electron bonds ) are well understood . the most energy dense chemicals , such as gasoline , store to ev per atom . cold fusion reactions have often produced far more than this , ranging from ~ to ~ , ev per atom of the material in the cell , and in all cases there is no chemical fuel before the reaction , and no chemical ash after it finishes . cell with a few grams of inert material in them have produced hundreds of megajoules of heat as much as several kilograms of gasoline . if you were to burn the cell contents , they would produce only a tiny fraction of the heat . in fact , you could burn the cell , the table , and all books in the room and still get that much chemical energy . therefore , chemistry is ruled out . . any calorimeter can measure an endothermic event as accurately as it measures an exothermic event . for example , all calorimeters used in these studies show the endothermic formation of pd-d at the beginning of the experiment . since the calorimeters shows the exothermic excess heat , they would also show the endothermic storage that proceeds it . they have never done this . the heat balance is always zero before the heat bursts , and after . - jed rothwell here is more useful information that someone keeps erasing : please note that cal tech ( lewis et al . ) is a false negative , as pointed out by noninski , miles and fleischmann . this is described in several papers , for example this one , p. : also , the name is julian schwinger ( with ch ) . see : - jed rothwell , librarian , lenr-canr.org ''\", ': thanks for the comment about wiki-defenderness . i like that one . i usually wikiling wiki-defender . i agree that at first he was somewhat innocent but now have my doubts as he is being really agressive about the whole matter .', \"it might not have been your intention , but your recent edit removed content from . please be careful not to remove content from without a valid reason , which you should specify in the edit summary or on the article 's talk page . take a look at our welcome page to learn more about contributing to this encyclopedia . thank you . a link to the edit i have reverted can be found here : link . if you believe this edit should not have been reverted , please contact me . ||suggestions ?\", \"agree , but that 's not the issue . the queston is , in everyday english , does first amendent refer primarily to the us constitution , or is it ambiguous ? my belief remains that even fifth amendment is ambiguous , and first amendment even more so . *\", \"`` == main towns that are not so main == i know that you love to write a one sentence article . but , if you want to do that , please , at least do it properly . stop saying `` '' x is a main town in y , z '' '' . what is so main about obscure towns ? ``\", '`` == halliday == good to see another contributor to his article . if sfl is your thing , you might consider joining the : wikiproject_linguistics/sflsfl taskforce . thanks. ``', \"`` : : : that stephen barrett is not board certified is not a viewpoint . it is a fact ( a well documented one at that ) . i do n't see how wp : prominence applies. ``\", ': could you do a precis of the material instead of just downloading a whole lot of stuff from somewhere else on the internet ? i am only one editor here , but i do feel that the material you have provided could be cut down to fit the format of this article . more info could be provided in a seperate article if necessary .', \"`` == cloud feedback == why is cloud feedback only under the positive feedbacks ? just about any paper states that it can be both . that the ipcc states that it is `` '' more likely a positive than a negative feedback '' '' does n't change that. ``\"]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have an instance of NGramPredictor called 'ngram_predictor' and it's already fit\n",
    "# And assuming 'sentences' is your list of test sentences:\n",
    "test_sentences = df_test['comment_text_word_tokenize_simple_normalization'].to_list()\n",
    "test_sentences = test_sentences[:50]\n",
    "print(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement et creation du model N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "tested_predictor = NGramPredictor(n=4)  # For trigrams\n",
    "tested_predictor.fit(sentences)  # Assuming 'sentences' is a list of sentence strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de l'accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "Context: thank you for - Expected: understanding Predicted: your 1\n",
      "Context: you for understanding - Expected: . Predicted: talk 1\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: i think very - Expected: highly Predicted: high 1\n",
      "None \n",
      "Context: very highly of - Expected: you Predicted: the 1\n",
      "None \n",
      "Context: of you and - Expected: would Predicted: your 1\n",
      "Context: you and would - Expected: not Predicted: not 1\n",
      "Context: and would not - Expected: revert Predicted: be 1\n",
      "Context: would not revert - Expected: without Predicted: a 1\n",
      "Context: not revert without - Expected: discussion Predicted: offering 1\n",
      "Context: revert without discussion - Expected: . Predicted: i 1\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: this site is - Expected: horrible Predicted: a 2\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: try to add - Expected: religion Predicted: some 3\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: the way people - Expected: have Predicted: who 3\n",
      "Context: way people have - Expected: invariably Predicted: already 3\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: and why do - Expected: you Predicted: you 3\n",
      "Context: why do you - Expected: bother Predicted: think 3\n",
      "Context: do you bother - Expected: bringing Predicted: at 3\n",
      "None \n",
      "None \n",
      "Context: bringing up the - Expected: long-dead Predicted: arguments 3\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: crap on the - Expected: fly Predicted: board 3\n",
      "Context: on the fly - Expected: . Predicted: a 3\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: acknowledgement in the - Expected: entire Predicted: lead 3\n",
      "Context: in the entire - Expected: amos Predicted: world 3\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: article that he - Expected: is Predicted: was 3\n",
      "Context: that he is - Expected: personally Predicted: a 3\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: is in the - Expected: categories Predicted: public 3\n",
      "Context: in the categories - Expected: ! Predicted: and 3\n",
      "None \n",
      "None \n",
      "Context: it says it - Expected: right Predicted: is 4\n",
      "Context: says it right - Expected: there Predicted: there 4\n",
      "Context: it right there - Expected: that Predicted: are 4\n",
      "Context: right there that - Expected: it Predicted: you 4\n",
      "Context: there that it - Expected: is Predicted: is 4\n",
      "Context: that it is - Expected: a Predicted: not 4\n",
      "Context: it is a - Expected: type Predicted: very 4\n",
      "Context: is a type - Expected: . Predicted: of 4\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: is needed in - Expected: this Predicted: the 4\n",
      "Context: needed in this - Expected: case Predicted: article 4\n",
      "Context: in this case - Expected: because Predicted: the 4\n",
      "Context: this case because - Expected: there Predicted: there 4\n",
      "Context: case because there - Expected: are Predicted: is 4\n",
      "Context: because there are - Expected: three Predicted: no 4\n",
      "Context: there are three - Expected: levels Predicted: articles 4\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: it is needed - Expected: in Predicted: for 4\n",
      "Context: is needed in - Expected: this Predicted: the 4\n",
      "Context: needed in this - Expected: case Predicted: article 4\n",
      "Context: in this case - Expected: to Predicted: the 4\n",
      "Context: this case to - Expected: clarify Predicted: add 4\n",
      "None \n",
      "Context: to clarify that - Expected: ub Predicted: the 4\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: it says it - Expected: even Predicted: is 4\n",
      "None \n",
      "Context: it even in - Expected: binghamton Predicted: indeed 4\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: state university of - Expected: new Predicted: new 4\n",
      "Context: university of new - Expected: york Predicted: york 4\n",
      "Context: of new york - Expected: , Predicted: city 4\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: stony brook university - Expected: . Predicted: in 4\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: stop trying to - Expected: say Predicted: be 4\n",
      "Context: trying to say - Expected: it Predicted: that 4\n",
      "Context: to say it - Expected: 's Predicted: s 4\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: not because i - Expected: am Predicted: am 4\n",
      "Context: because i am - Expected: totally Predicted: a 4\n",
      "Context: i am totally - Expected: right Predicted: in 4\n",
      "Context: am totally right - Expected: in Predicted: but 4\n",
      "None \n",
      "Context: right in this - Expected: case Predicted: case 4\n",
      "Context: in this case - Expected: . Predicted: the 4\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: before adding a - Expected: new Predicted: new 5\n",
      "Context: adding a new - Expected: product Predicted: abuse 5\n",
      "Context: a new product - Expected: to Predicted: at 5\n",
      "None \n",
      "None \n",
      "Context: to the list - Expected: , Predicted: of 5\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: make sure it - Expected: 's Predicted: s 5\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: before adding a - Expected: new Predicted: new 5\n",
      "Context: adding a new - Expected: product Predicted: abuse 5\n",
      "Context: a new product - Expected: to Predicted: at 5\n",
      "None \n",
      "None \n",
      "Context: to the list - Expected: , Predicted: of 5\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: make sure it - Expected: has Predicted: s 5\n",
      "Context: sure it has - Expected: a Predicted: consensus 5\n",
      "Context: it has a - Expected: entry Predicted: lot 5\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: and giving the - Expected: reader Predicted: facts 5\n",
      "Context: giving the reader - Expected: the Predicted: a 5\n",
      "Context: the reader the - Expected: possibility Predicted: wrong 5\n",
      "None \n",
      "Context: the possibility to - Expected: read Predicted: contact 5\n",
      "None \n",
      "Context: to read more - Expected: about Predicted: about 5\n",
      "Context: read more about - Expected: it Predicted: the 5\n",
      "Context: more about it - Expected: . Predicted: than 5\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: otherwise it could - Expected: be Predicted: be 5\n",
      "Context: it could be - Expected: subject Predicted: a 5\n",
      "Context: could be subject - Expected: to Predicted: for 5\n",
      "Context: be subject to - Expected: deletion Predicted: the 5\n",
      "Context: subject to deletion - Expected: . Predicted: as 5\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: see this article - Expected: 's Predicted: is 5\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: this article needs - Expected: a Predicted: to 7\n",
      "Context: article needs a - Expected: section Predicted: lot 7\n",
      "Context: needs a section - Expected: on Predicted: on 7\n",
      "Context: a section on - Expected: /why/ Predicted: the 7\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: at the moment - Expected: , Predicted: i 7\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: it seems kind - Expected: of Predicted: of 7\n",
      "Context: seems kind of - Expected: arbitrary Predicted: strange 7\n",
      "Context: kind of arbitrary - Expected: . Predicted: neutral 7\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: burn in hell - Expected: . Predicted: for 9\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: if you continue - Expected: to Predicted: to 10\n",
      "Context: you continue to - Expected: vandalize Predicted: do 10\n",
      "Context: continue to vandalize - Expected: , Predicted: pages 10\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: as you did - Expected: to Predicted: with 10\n",
      "Context: you did to - Expected: homosexuality Predicted: the 10\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: you will be - Expected: blocked Predicted: blocked 10\n",
      "Context: will be blocked - Expected: from Predicted: from 10\n",
      "Context: be blocked from - Expected: editing Predicted: editing 10\n",
      "Context: blocked from editing - Expected: . Predicted: talk 10\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: i have edited - Expected: the Predicted: the 11\n",
      "Context: have edited the - Expected: introduction Predicted: article 11\n",
      "Context: edited the introduction - Expected: , Predicted: for 11\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: previously it said - Expected: that Predicted: that 11\n",
      "Context: it said that - Expected: passive Predicted: the 11\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: does not use - Expected: any Predicted: the 11\n",
      "Context: not use any - Expected: kind Predicted: of 11\n",
      "None \n",
      "Context: any kind of - Expected: energy Predicted: written 11\n",
      "Context: kind of energy - Expected: . Predicted: into 11\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: this is not - Expected: true Predicted: a 11\n",
      "Context: is not true - Expected: . Predicted: i 11\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: relies on the - Expected: kinetic Predicted: hope 11\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: energy of the - Expected: substance Predicted: electromagnetic 11\n",
      "Context: of the substance - Expected: that Predicted: is 11\n",
      "Context: the substance that - Expected: is Predicted: the 11\n",
      "Context: substance that is - Expected: being Predicted: beyond 11\n",
      "Context: that is being - Expected: transported Predicted: discussed 11\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: is what causes - Expected: it Predicted: a 11\n",
      "Context: what causes it - Expected: to Predicted: and 11\n",
      "Context: causes it to - Expected: move Predicted: crack 11\n",
      "Context: it to move - Expected: around Predicted: on 11\n",
      "Context: to move around - Expected: and Predicted: it 11\n",
      "Context: move around and - Expected: ( Predicted: could 11\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: the difference is - Expected: that Predicted: that 11\n",
      "Context: difference is that - Expected: active Predicted: the 11\n",
      "Context: is that active - Expected: transport Predicted: banana 11\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: actually uses the - Expected: cell Predicted: article 11\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: cut it short - Expected: . Predicted: here 12\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: if you have - Expected: sources Predicted: any 12\n",
      "Context: you have sources - Expected: stating Predicted: that 12\n",
      "None \n",
      "Context: sources stating the - Expected: rok Predicted: irish 12\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: wp is not - Expected: the Predicted: a 12\n",
      "Context: is not the - Expected: place Predicted: case 12\n",
      "Context: not the place - Expected: to Predicted: for 12\n",
      "Context: the place to - Expected: make Predicted: discuss 12\n",
      "Context: place to make - Expected: or Predicted: a 12\n",
      "Context: to make or - Expected: . Predicted: any 12\n",
      "None \n",
      "Context: jews are not - Expected: a Predicted: actually 13\n",
      "Context: are not a - Expected: race Predicted: native 13\n",
      "Context: not a race - Expected: because Predicted: or 13\n",
      "None \n",
      "None \n",
      "Context: because you can - Expected: only Predicted: not 13\n",
      "Context: you can only - Expected: get Predicted: copy 13\n",
      "Context: can only get - Expected: it Predicted: the 13\n",
      "None \n",
      "Context: get it from - Expected: your Predicted: a 13\n",
      "Context: it from your - Expected: mother Predicted: talk 13\n",
      "Context: from your mother - Expected: . Predicted: who 13\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: proves it is - Expected: not Predicted: a 13\n",
      "Context: it is not - Expected: , Predicted: a 13\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: as well as - Expected: the Predicted: the 13\n",
      "Context: well as the - Expected: fact Predicted: guidelines 13\n",
      "Context: as the fact - Expected: that Predicted: that 13\n",
      "Context: the fact that - Expected: we Predicted: the 13\n",
      "Context: fact that we - Expected: accept Predicted: are 13\n",
      "Context: that we accept - Expected: converts Predicted: that 13\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: or others think - Expected: that Predicted: was 14\n",
      "Context: others think that - Expected: one Predicted: it 14\n",
      "Context: think that one - Expected: list Predicted: of 14\n",
      "None \n",
      "Context: one list of - Expected: the Predicted: organized 14\n",
      "Context: list of the - Expected: oldest Predicted: o 14\n",
      "Context: of the oldest - Expected: people Predicted: roman 14\n",
      "None \n",
      "None \n",
      "Context: people we know - Expected: about Predicted: really 14\n",
      "Context: we know about - Expected: is Predicted: the 14\n",
      "Context: know about is - Expected: too Predicted: impossible 14\n",
      "None \n",
      "Context: is too long - Expected: , Predicted: and 14\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: answer is to - Expected: raise Predicted: keep 14\n",
      "Context: is to raise - Expected: the Predicted: anthrocon 14\n",
      "Context: to raise the - Expected: cutoff Predicted: issue 14\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: is purely a - Expected: round Predicted: matter 14\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: number and a - Expected: full Predicted: designation 14\n",
      "Context: and a full - Expected: years Predicted: bio 14\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: we can make - Expected: it Predicted: it 14\n",
      "Context: can make it - Expected: the Predicted: a 14\n",
      "Context: make it the - Expected: top Predicted: same 14\n",
      "Context: it the top - Expected: or Predicted: gear 14\n",
      "Context: the top or - Expected: top Predicted: a 14\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: tell us what - Expected: the Predicted: username 14\n",
      "Context: us what the - Expected: maximum Predicted: album 14\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: is and we - Expected: can Predicted: re 14\n",
      "Context: and we can - Expected: set Predicted: discuss 14\n",
      "Context: we can set - Expected: a Predicted: a 14\n",
      "Context: can set a - Expected: threshold Predicted: date 14\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: not sure whether - Expected: this Predicted: you 16\n",
      "Context: sure whether this - Expected: is Predicted: particular 16\n",
      "Context: whether this is - Expected: notable Predicted: a 16\n",
      "Context: this is notable - Expected: enough Predicted: enough 16\n",
      "Context: is notable enough - Expected: to Predicted: to 16\n",
      "Context: notable enough to - Expected: be Predicted: have 16\n",
      "Context: enough to be - Expected: mentioned Predicted: included 16\n",
      "Context: to be mentioned - Expected: in Predicted: in 16\n",
      "Context: be mentioned in - Expected: the Predicted: the 16\n",
      "Context: mentioned in the - Expected: article Predicted: article 16\n",
      "Context: in the article - Expected: , Predicted: i 16\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: in its own - Expected: format Predicted: right 16\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: with a suffix - Expected: of Predicted: identifying 16\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: where x is - Expected: the Predicted: or 16\n",
      "Context: x is the - Expected: version Predicted: name 16\n",
      "Context: is the version - Expected: number Predicted: i 16\n",
      "Context: the version number - Expected: ( Predicted: here 16\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: this is intentional - Expected: , Predicted: and 16\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: this page has - Expected: one Predicted: been 18\n",
      "Context: page has one - Expected: sentence Predicted: edit 18\n",
      "Context: has one sentence - Expected: about Predicted: about 18\n",
      "Context: one sentence about - Expected: the Predicted: the 18\n",
      "Context: sentence about the - Expected: basic Predicted: positive 18\n",
      "Context: about the basic - Expected: definition Predicted: concept 18\n",
      "Context: the basic definition - Expected: of Predicted: of 18\n",
      "Context: basic definition of - Expected: the Predicted: military 18\n",
      "Context: definition of the - Expected: word Predicted: word 18\n",
      "Context: of the word - Expected: , Predicted: in 18\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: and a huge - Expected: amount Predicted: lie 18\n",
      "Context: a huge amount - Expected: about Predicted: of 18\n",
      "None \n",
      "Context: amount about the - Expected: slang/profane Predicted: articles 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: should be extended - Expected: ; Predicted: to 18\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: is there no - Expected: information Predicted: mention 18\n",
      "Context: there no information - Expected: about Predicted: about 18\n",
      "Context: no information about - Expected: female Predicted: it 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: this is an - Expected: encyclopaedia Predicted: ip 18\n",
      "Context: is an encyclopaedia - Expected: , Predicted: not 18\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: not a dictionary - Expected: . Predicted: definition 18\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: i feel that - Expected: whoever Predicted: the 18\n",
      "None \n",
      "Context: that whoever is - Expected: looking Predicted: against 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: and should be - Expected: deleted Predicted: removed 18\n",
      "Context: should be deleted - Expected: from Predicted: please 18\n",
      "Context: be deleted from - Expected: this Predicted: the 18\n",
      "Context: deleted from this - Expected: word Predicted: has 18\n",
      "Context: from this word - Expected: is Predicted: yaktubu 18\n",
      "Context: this word is - Expected: used Predicted: used 18\n",
      "Context: word is used - Expected: very Predicted: in 18\n",
      "Context: is used very - Expected: often Predicted: often 18\n",
      "Context: used very often - Expected: and Predicted: in 18\n",
      "Context: very often and - Expected: is Predicted: certainly 18\n",
      "None \n",
      "Context: and is also - Expected: a Predicted: supported 18\n",
      "Context: is also a - Expected: very Predicted: votes 18\n",
      "Context: also a very - Expected: `` Predicted: good 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: i belive that - Expected: is Predicted: i 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: very much so - Expected: . Predicted: i 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: meaning is a - Expected: female Predicted: form 18\n",
      "Context: is a female - Expected: dog Predicted: her 18\n",
      "Context: a female dog - Expected: . Predicted: in 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: stands for the - Expected: name Predicted: fact 18\n",
      "Context: for the name - Expected: brittany Predicted: of 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: preceding unsigned comment - Expected: added Predicted: added 18\n",
      "Context: unsigned comment added - Expected: by Predicted: by 18\n",
      "Context: comment added by - Expected: ==etymology== Predicted: talk 18\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: the word bitch - Expected: is Predicted: as 18\n",
      "None \n",
      "None \n",
      "Context: is from the - Expected: old Predicted: same 18\n",
      "Context: from the old - Expected: norse Predicted: a 18\n",
      "Context: the old norse - Expected: bikkjuna Predicted: ne 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: of the dog - Expected: '' Predicted: is 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: of unknown origin - Expected: , Predicted: and 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: the old norse - Expected: words Predicted: ne 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: the converse is - Expected: equally Predicted: not 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: was first seen - Expected: in Predicted: in 18\n",
      "Context: first seen in - Expected: . Predicted: the 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: to complain in - Expected: . Predicted: arbcom 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: the word bitch - Expected: == Predicted: as 18\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: the word bitch - Expected: is Predicted: as 18\n",
      "None \n",
      "None \n",
      "Context: is actually only - Expected: offencive Predicted: his 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: american and canadian - Expected: english Predicted: anthropologists 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: in most other - Expected: english Predicted: relevant 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: people have argued - Expected: that Predicted: that 18\n",
      "Context: have argued that - Expected: bitch Predicted: this 18\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: is different because - Expected: it Predicted: it 18\n",
      "Context: different because it - Expected: is Predicted: declared 18\n",
      "Context: because it is - Expected: used Predicted: a 18\n",
      "Context: it is used - Expected: as Predicted: in 18\n",
      "Context: is used as - Expected: an Predicted: a 18\n",
      "Context: used as an - Expected: insult Predicted: excuse 18\n",
      "Context: as an insult - Expected: - Predicted: but 18\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: but so is - Expected: `` Predicted: the 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: these are not - Expected: considered Predicted: the 18\n",
      "Context: are not considered - Expected: profane Predicted: reliable 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: so why is - Expected: bitch Predicted: it 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "Context: as far as - Expected: i Predicted: i 18\n",
      "Context: far as i - Expected: am Predicted: can 18\n",
      "Context: as i am - Expected: concerned Predicted: not 18\n",
      "Context: i am concerned - Expected: , Predicted: that 18\n",
      "None \n",
      "None \n",
      "None \n",
      "Context: there is nothing - Expected: to Predicted: wrong 18\n",
      "Context: is nothing to - Expected: `` Predicted: do 18\n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n",
      "None \n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_model(tested_predictor, test_sentences)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
