\chapter{Tokenizer}

\section{Pré-traitement du jeu de données}

Le pré-traitement des données est une étape essentielle dans le processus d'analyse de texte. Il vise à nettoyer, transformer et préparer les données brutes afin de les rendre exploitables pour l'entraînement des modèles. Dans cette section, nous appliquerons différentes techniques de pré-traitement sur notre jeu de données afin de le rendre apte à être utilisé dans nos modèles prédictifs.

\subsection{Tokenisation à base d’expressions régulières (RegexTokenizer)}

La tokenisation est le processus de division du texte en unités plus petites appelées "tokens". La tokenisation à base d’expressions régulières utilise des règles basées sur des motifs d'expressions régulières pour diviser le texte en tokens.

\subsubsection*{Implémentation de la tokenisation à l'aide de la classe RegexTokenizer}

Nous commençons par importer la classe RegexTokenizer et appliquer la tokenisation sur notre jeu de données.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Avant tokenisation} & \textbf{Après tokenisation} \\ \hline
``Hello, World!'' & ['Hello', ',', 'World', '!'] \\ \hline
``I love NLTK'' & ['I', 'love', 'NLTK'] \\ \hline
\end{tabular}
\caption{Exemple de tokenisation à base d'expressions régulières}
\end{table}

\subsection{Tokenisation byte-pair encoding (TikToken)}

Le byte-pair encoding (BPE) est une méthode de tokenisation qui découpe le texte en sous-unités de texte appelées "tokens" en utilisant un algorithme de compression de données.

\subsubsection*{Implémentation de la tokenisation à l'aide de la bibliothèque TikToken}

Nous appliquons la tokenisation BPE sur notre jeu de données en utilisant la bibliothèque TikToken.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Avant tokenisation} & \textbf{Après tokenisation} \\ \hline
``Hello, World!'' & ['Hello', ',', 'World', '!'] \\ \hline
``I love TikToken'' & ['I', 'love', 'Ti', 'k', 'To', 'ken'] \\ \hline
\end{tabular}
\caption{Exemple de tokenisation BPE}
\end{table}

\subsection{Comparaison avec d'autres tokenizers}

Nous avons également comparé la performance de notre tokenizer avec d'autres options disponibles dans la bibliothèque MinBPE.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Tokenizer} & \textbf{Temps d'entrainement (minutes)} \\ \hline
RegexTokenizer & 182 \\ \hline
TikToken & 0 (déja entrainé) \\ \hline
BasicTokenizer & 69 \\ \hline
\end{tabular}
\caption{Comparaison des temps d'entrainement des tokenizers}
\end{table}


Nous constatons que le RegexTokenizer et le TikToken sont significativement plus rapides que le BasicTokenizer. TikToken est le plus rapide parmi tous les tokenizers testés.

\subsection{Méthodes de normalisation du texte}
La normalisation du texte est une étape cruciale du pré-traitement des données textuelles. Elle vise à uniformiser le texte en le mettant en minuscules, en supprimant les stop words et en lemmatisant les mots.

\subsubsection*{Suppression des stop words}

Les stop words sont des mots courants qui n'apportent pas beaucoup de valeur sémantique au texte(exemple : 'a' , 'the' ,'for',...). Nous les supprimerons de notre jeu de données.

\subsubsection*{Lemmatisation}

La lemmatisation consiste à réduire les mots fléchis ou dérivés à leur forme de base ou racine. Cela permet de normaliser le texte et de réduire la dimensionnalité de l'espace des features.

\subsubsection*{Mise en minuscules}

La mise en minuscules permet d'uniformiser le texte en convertissant toutes les lettres en minuscules. Cela permet d'éviter les doublons dus à la casse.

\subsubsection*{Identification des verbes et des noms}

Nous avons modifié la méthode de lemmatisation pour identifier les verbes et les noms dans une phrase. Parfois, les verbes étaient considérés comme des noms et ne pouvaient pas être lemmatisés avec le lemmatiseur par défaut.

\subsubsection*{Les caractères spéciaux}

En examinant attentivement notre base de données, nous avons constaté la présence de nombreux tokens représentant des URL, des adresses e-mail et d’autres éléments inutiles pour nos calculs d’entraînement des modèles. 
Par conséquent, nous avons décidé de les supprimer afin de réduire au maximum le bruit dans nos données et ainsi optimiser nos modèles.

\subsubsection*{Remplacement des emojis}

Certains emojis peuvent etre important pour l'indication d'une toxicité ou non c'est pourquoi nous avons pris la décision de remplacer certains emojis par un mot le représentant au mieux afin de permettre aux models de mieux s'adapter.

\subsubsection*{Effacement des ponctuations}

Parfois, certaines ponctuations occupent trop d’espace sans être utiles au modèle. Ainsi, nous pouvons supprimer tous les caractères qui ne sont pas dans la table ASCII ainsi que les signes de ponctuation qui pourraient compromettre l’entraînement des modèles.

\subsubsection*{Effacement des duplication}
En constatant que plusieurs messages vulgaires étaient dissimulés parmi nos tokens en raison de la duplication des premières et/ou dernières lettres, nous avons été contraints de corriger ces tokens pour leur attribuer une plus grande valeur.

\subsubsection*{Tableau représentatif des différentes fonctions de pré-traitement}
\begin{table}[ht]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \renewcommand{\arraystretch}{1.5} % Increases row height
    \setlength{\tabcolsep}{6pt} % Adjusts column padding
    \begin{tabular}{|p{3.5cm}|c|c|c|c|c|c|c|}
        \hline
        \textbf{Pré traitement} & \textbf{Remove special caractère} & \textbf{Remove Stop W} & \textbf{Replace emojis} & \textbf{Lowercase} & \textbf{Lemmatization} & \textbf{Remove punctuation} & \textbf{Remove duplication} \\
        \hline
        BPE Tokenizer 2  & VRAI & FAUX & FAUX & VRAI & FAUX & VRAI & VRAI \\
        BPE Tokenizer 1  & VRAI & FAUX & FAUX & VRAI & FAUX & FAUX & VRAI \\
        Word Tokenizer 4  & VRAI & VRAI & VRAI & VRAI & VRAI & VRAI & VRAI \\
        Word Tokenizer 3 & VRAI & VRAI & VRAI & VRAI & VRAI & FAUX & FAUX \\
        Word Tokenizer 2  & VRAI & FAUX & FAUX & VRAI & FAUX & VRAI & FAUX \\
        Word Tokenizer 1  & FAUX & FAUX & FAUX & FAUX & FAUX & FAUX & FAUX \\
        GPT Tokenizer 4  & VRAI & VRAI & VRAI & VRAI & VRAI & VRAI & VRAI \\
        GPT Tokenizer 3 & VRAI & VRAI & VRAI & VRAI & VRAI & FAUX & FAUX \\
        GPT Tokenizer 2  & VRAI & FAUX & FAUX & VRAI & FAUX & FAUX & FAUX \\
        GPT Tokenizer 1 & FAUX & FAUX & FAUX & FAUX & FAUX & FAUX & FAUX \\
        Baseline & FAUX & FAUX & FAUX & FAUX & FAUX & FAUX & FAUX \\
        \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Pré traitement comparatif}
    \label{tab:pre_treatment_comparative}
\end{table}

\subsection{Pré-traitement supplémentaire du jeu de données}

En plus des techniques de pré-traitement déjà mentionnées, nous avons également effectué les actions suivantes pour rendre notre jeu de données plus adapté à l'entraînement de nos modèles :

\subsubsection*{Réflexion sur la suppression des symboles opérateurs et des chiffres}

Dans le cadre du prétraitement des données, nous avons envisagé initialement de supprimer les symboles opérateurs et les chiffres présents dans nos textes. Cependant, après réflexion, nous avons réalisé que ces symboles pouvaient avoir une valeur sémantique importante pour déterminer si une phrase est toxique ou non. Ainsi, au lieu de les supprimer, nous avons mis en place un processus sophistiqué consistant à utiliser une série d'expressions régulières (regex) pour remplacer ces symboles par leur signification correspondante. Par exemple, nous avons remplacé des symboles comme ":/" par leur signification afin d'améliorer la compréhension du texte par nos modèles. Cette approche nous permet d'assurer des interactions plus contextuelles et pertinentes avec les utilisateurs, tout en garantissant une analyse précise et efficace du langage. Chacune de ces expressions régulières a été soigneusement personnalisée par notre équipe pour répondre aux besoins spécifiques de notre projet, permettant ainsi à notre IA de mieux comprendre le langage naturel et d'interagir de manière plus fluide avec les utilisateurs.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Élément toxique} & \textbf{Fréquence} \\ \hline
8======d~~ & penis \\ \hline
:)))) & happy \\ \hline
:( & sad \\ \hline
:) & happy \\ \hline
:D & laught \\ \hline
8============> & penis \\ \hline
\end{tabular}
\caption{Exemple d'éléments toxiques contenant des symboles opérateurs et des chiffres que nous avons remplacé par leurs signification}
\end{table}


\subsubsection*{Suppression des balises HTML, des URL et des adresses e-mail}

Nous avons supprimé les balises HTML, les URL et les adresses e-mail de notre jeu de données. Ces éléments étaient fréquents dans notre base de données en raison d'erreurs de récupération de données lors de la collecte de commentaires sur Wikipedia.

\subsubsection*{Correction des erreurs orthographiques et des fautes de frappe}

Nous avons utilisé un algorithme de correction des erreurs orthographiques et des fautes de frappe pour améliorer la qualité de nos données textuelles. Cet algorithme remplace les mots incorrectement orthographiés par les mots les plus proches dans notre dictionnaire de référence. Par exemple les caractères spéciaux, tels que 'ı' utilisé à la place de 'i', seront corrigés. Voici des exemples de phrases avant et après l'application de notre algorithme de correction :

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Avant correction} & \textbf{Après correction} \\ \hline
    lıes  & lies \\ \hline
    penisd i'\textbackslash m & penis i'm  \\ \hline
    fùck8 & fuck  \\ \hline
    \end{tabular}
    \caption{Exemple de phrase avant et après correction}
\end{table}

\subsubsection*{Non-application de la correction d'orthographe}
Initialement, nous avions envisagé d'appliquer un algorithme de correction d'orthographe pour améliorer la qualité de nos données textuelles. Cependant, après avoir réalisé que cette étape prenait trop de temps et pouvait introduire des erreurs, notamment en modifiant des noms propres en mots n'ayant aucun rapport, nous avons décidé de ne pas l'appliquer. En effet, cela pourrait introduire des biais importants, notamment si un prénom ressemble à une insulte, ce qui pourrait transformer une phrase anodine en une phrase insultante. Par conséquent, nous avons choisi de ne pas effectuer cette étape de correction d'orthographe pour garantir l'intégrité et la qualité de nos données textuelles.

\subsubsection*{Exemple d'une phrase après toutes les étapes de normalisation}

\begin{table}[h]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|>{\raggedright\arraybackslash}m{5cm}|>{\raggedright\arraybackslash}m{15cm}|}
        \hline
        \textbf{\fontsize{14}{16}\selectfont Étape} & \textbf{\fontsize{14}{16}\selectfont Texte normalisé} \\ \hline
        \fontsize{14}{16}\selectfont Texte initial & \fontsize{14}{16}\selectfont motherrrrrrrrrrrrFuckkkkkkkk!!!!aaaaaaa***** stringssss ***!!!!!!!bitchhhhhh :) 8========= \\ \hline
        \fontsize{14}{16}\selectfont Après remplacement des emojis & \fontsize{14}{16}\selectfont motherrrrrrrrrrrrFuckkkkkkkk!!!!aaaaaaa***** stringssss ***!!!!!!!bitchhhhhh happy 8========= \\ \hline
        \fontsize{14}{16}\selectfont Après suppression des caractères spéciaux & \fontsize{14}{16}\selectfont motherFuck a strings bitch happy penis \\ \hline
        \fontsize{14}{16}\selectfont Après tokenisation & \fontsize{14}{16}\selectfont 'motherFuck', 'a', 'strings', 'bitch', 'happy', 'penis' \\ \hline
        \fontsize{14}{16}\selectfont Après conversion en minuscules & \fontsize{14}{16}\selectfont 'motherfuck', 'a', 'strings', 'bitch', 'happy', 'penis' \\ \hline
        \fontsize{14}{16}\selectfont Après lemmatisation & \fontsize{14}{16}\selectfont 'motherfuck', 'a', 'string', 'bitch', 'happy', 'penis' \\ \hline
        \fontsize{14}{16}\selectfont Après suppression des stopwords & \fontsize{14}{16}\selectfont 'motherfuck', 'string', 'bitch', 'happy', 'penis' \\ \hline
        \fontsize{14}{16}\selectfont Texte final après re-tokenisation & \fontsize{14}{16}\selectfont mother fuck string bitch happy penis \\ \hline
    \end{tabular}%
    }
    \caption{Étapes de normalisation du texte}
\end{table}