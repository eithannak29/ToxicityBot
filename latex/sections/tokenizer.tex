\chapter{Tokenizer}

\section{Pré-traitement du jeu de données}

Le pré-traitement des données est une étape essentielle dans le processus d'analyse de texte. Il vise à nettoyer, transformer et préparer les données brutes afin de les rendre exploitables pour l'entraînement des modèles. Dans cette section, nous appliquerons différentes techniques de pré-traitement sur notre jeu de données afin de le rendre apte à être utilisé dans nos modèles prédictifs.

\subsection{Tokenisation à base d’expressions régulières (RegexTokenizer)}

La tokenisation est le processus de division du texte en unités plus petites appelées "tokens". La tokenisation à base d’expressions régulières utilise des règles basées sur des motifs d'expressions régulières pour diviser le texte en tokens.

\subsubsection*{Description de la méthode de tokenisation à base d'expressions régulières}

La tokenisation à base d'expressions régulières consiste à diviser le texte en tokens en utilisant des règles définies par des motifs d'expressions régulières. Ces motifs permettent de reconnaître les limites entre les mots, les ponctuations, etc.

\subsubsection*{Implémentation de la tokenisation à l'aide de la classe RegexTokenizer}

Nous commençons par importer la classe RegexTokenizer et appliquer la tokenisation sur notre jeu de données.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Avant tokenisation} & \textbf{Après tokenisation} \\ \hline
``Hello, World!'' & ['Hello', ',', 'World', '!'] \\ \hline
``I love NLTK'' & ['I', 'love', 'NLTK'] \\ \hline
\end{tabular}
\caption{Exemple de tokenisation à base d'expressions régulières}
\end{table}

\subsection{Tokenisation byte-pair encoding (TikToken)}

Le byte-pair encoding (BPE) est une méthode de tokenisation qui découpe le texte en sous-unités de texte appelées "tokens" en utilisant un algorithme de compression de données.

\subsubsection*{Description de la méthode de tokenisation byte-pair encoding}

La tokenisation BPE découpe le texte en sous-unités de texte de taille variable, appelées "tokens", en utilisant un algorithme de compression de données.

\subsubsection*{Implémentation de la tokenisation à l'aide de la bibliothèque TikToken}

Nous appliquons la tokenisation BPE sur notre jeu de données en utilisant la bibliothèque TikToken.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Avant tokenisation} & \textbf{Après tokenisation} \\ \hline
``Hello, World!'' & ['Hello', ',', 'World', '!'] \\ \hline
``I love TikToken'' & ['I', 'love', 'Ti', 'k', 'To', 'ken'] \\ \hline
\end{tabular}
\caption{Exemple de tokenisation BPE}
\end{table}

\subsection{Comparaison avec d'autres tokenizers}

Nous avons également comparé la performance de notre tokenizer avec d'autres options disponibles dans la bibliothèque MinBPE.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Tokenizer} & \textbf{Temps d'exécution (minutes)} \\ \hline
RegexTokenizer & ??? \\ \hline
TikToken & 1 \\ \hline
BasicTokenizer & 69 \\ \hline
\end{tabular}
\caption{Comparaison des temps d'exécution des tokenizers}
\end{table}

Nous constatons que le RegexTokenizer et le TikToken sont significativement plus rapides que le BasicTokenizer. TikToken est le plus rapide parmi tous les tokenizers testés.

\subsection{Méthodes de normalisation du texte}

La normalisation du texte est une étape cruciale du pré-traitement des données textuelles. Elle vise à uniformiser le texte en le mettant en minuscules, en supprimant les stop words et en lemmatisant les mots.

\subsubsection*{Suppression des stop words}

Les stop words sont des mots courants qui n'apportent pas beaucoup de valeur sémantique au texte. Nous les supprimerons de notre jeu de données.

\subsubsection*{Lemmatisation}

La lemmatisation consiste à réduire les mots fléchis ou dérivés à leur forme de base ou racine. Cela permet de normaliser le texte et de réduire la dimensionnalité de l'espace des features.

\subsubsection*{Mise en minuscules}

La mise en minuscules permet d'uniformiser le texte en convertissant toutes les lettres en minuscules. Cela permet d'éviter les doublons dus à la casse.

\end{document}
