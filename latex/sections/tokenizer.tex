\chapter{Tokenizer}

\section{Pré-traitement du jeu de données}

Le pré-traitement des données est une étape essentielle dans le processus d'analyse de texte. Il vise à nettoyer, transformer et préparer les données brutes afin de les rendre exploitables pour l'entraînement des modèles. Dans cette section, nous appliquerons différentes techniques de pré-traitement sur notre jeu de données afin de le rendre apte à être utilisé dans nos modèles prédictifs.

\subsection{Tokenisation à base d’expressions régulières (RegexTokenizer)}

La tokenisation est le processus de division du texte en unités plus petites appelées "tokens". La tokenisation à base d’expressions régulières utilise des règles basées sur des motifs d'expressions régulières pour diviser le texte en tokens.

\subsubsection*{Description de la méthode de tokenisation à base d'expressions régulières}

La tokenisation à base d'expressions régulières consiste à diviser le texte en tokens en utilisant des règles définies par des motifs d'expressions régulières. Ces motifs permettent de reconnaître les limites entre les mots, les ponctuations, etc.

\subsubsection*{Implémentation de la tokenisation à l'aide de la classe RegexTokenizer}

Nous commençons par importer la classe RegexTokenizer et appliquer la tokenisation sur notre jeu de données.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Avant tokenisation} & \textbf{Après tokenisation} \\ \hline
``Hello, World!'' & ['Hello', ',', 'World', '!'] \\ \hline
``I love NLTK'' & ['I', 'love', 'NLTK'] \\ \hline
\end{tabular}
\caption{Exemple de tokenisation à base d'expressions régulières}
\end{table}

\subsection{Tokenisation byte-pair encoding (TikToken)}

Le byte-pair encoding (BPE) est une méthode de tokenisation qui découpe le texte en sous-unités de texte appelées "tokens" en utilisant un algorithme de compression de données.

\subsubsection*{Description de la méthode de tokenisation byte-pair encoding}

La tokenisation BPE découpe le texte en sous-unités de texte de taille variable, appelées "tokens", en utilisant un algorithme de compression de données.

\subsubsection*{Implémentation de la tokenisation à l'aide de la bibliothèque TikToken}

Nous appliquons la tokenisation BPE sur notre jeu de données en utilisant la bibliothèque TikToken.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Avant tokenisation} & \textbf{Après tokenisation} \\ \hline
``Hello, World!'' & ['Hello', ',', 'World', '!'] \\ \hline
``I love TikToken'' & ['I', 'love', 'Ti', 'k', 'To', 'ken'] \\ \hline
\end{tabular}
\caption{Exemple de tokenisation BPE}
\end{table}

\subsection{Comparaison avec d'autres tokenizers}

Nous avons également comparé la performance de notre tokenizer avec d'autres options disponibles dans la bibliothèque MinBPE.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Tokenizer} & \textbf{Temps d'entrainement (minutes)} \\ \hline
RegexTokenizer & ??? \\ \hline
TikToken & 0 (déja entrainé) \\ \hline
BasicTokenizer & 69 \\ \hline
\end{tabular}
\caption{Comparaison des temps d'entrainement des tokenizers}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Tokenizer} & \textbf{Temps d'éxecution (seconde)} \\ \hline
RegexTokenizer & ??? \\ \hline
TikToken & ??? \\ \hline
BasicTokenizer & ??? \\ \hline
\end{tabular}
\caption{Comparaison des temps d'éxecution des tokenizers}
\end{table}


Nous constatons que le RegexTokenizer et le TikToken sont significativement plus rapides que le BasicTokenizer. TikToken est le plus rapide parmi tous les tokenizers testés.

\subsection{Méthodes de normalisation du texte}

La normalisation du texte est une étape cruciale du pré-traitement des données textuelles. Elle vise à uniformiser le texte en le mettant en minuscules, en supprimant les stop words et en lemmatisant les mots.

\subsubsection*{Suppression des stop words}

Les stop words sont des mots courants qui n'apportent pas beaucoup de valeur sémantique au texte. Nous les supprimerons de notre jeu de données.

\subsubsection*{Lemmatisation}

La lemmatisation consiste à réduire les mots fléchis ou dérivés à leur forme de base ou racine. Cela permet de normaliser le texte et de réduire la dimensionnalité de l'espace des features.

\subsubsection*{Mise en minuscules}

La mise en minuscules permet d'uniformiser le texte en convertissant toutes les lettres en minuscules. Cela permet d'éviter les doublons dus à la casse.

\subsubsection*{Identification des verbes et des noms}

Nous avons modifié la méthode de lemmatisation pour identifier les verbes et les noms dans une phrase. Parfois, les verbes étaient considérés comme des noms et ne pouvaient pas être lemmatisés avec le lemmatiseur par défaut.

\subsection{Pré-traitement supplémentaire du jeu de données}

En plus des techniques de pré-traitement déjà mentionnées, nous avons également effectué les actions suivantes pour rendre notre jeu de données plus adapté à l'entraînement de nos modèles :

\subsubsection*{Réflexion sur la suppression des symboles opérateurs et des chiffres}

Dans le cadre du prétraitement des données, nous avons envisagé initialement de supprimer les symboles opérateurs et les chiffres présents dans nos textes. Cependant, après réflexion, nous avons réalisé que ces symboles pouvaient avoir une valeur sémantique importante pour déterminer si une phrase est toxique ou non. Ainsi, au lieu de les supprimer, nous avons mis en place un processus sophistiqué consistant à utiliser une série d'expressions régulières (regex) pour remplacer ces symboles par leur signification correspondante. Par exemple, nous avons remplacé des symboles comme ":/" par leur signification afin d'améliorer la compréhension du texte par nos modèles. Cette approche nous permet d'assurer des interactions plus contextuelles et pertinentes avec les utilisateurs, tout en garantissant une analyse précise et efficace du langage. Chacune de ces expressions régulières a été soigneusement personnalisée par notre équipe pour répondre aux besoins spécifiques de notre projet, permettant ainsi à notre IA de mieux comprendre le langage naturel et d'interagir de manière plus fluide avec les utilisateurs.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Élément toxique} & \textbf{Fréquence} \\ \hline
8======d~~ & dick \\ \hline
x)))) & happy \\ \hline
:/ & sad \\ \hline
x) & happy \\ \hline
:D & laught \\ \hline
8============> & dick \\ \hline
\end{tabular}
\caption{Exemple d'éléments toxiques contenant des symboles opérateurs et des chiffres que nous avons remplacé par leurs signification}
\end{table}


\subsubsection*{Suppression des balises HTML, des URL et des adresses e-mail}

Nous avons supprimé les balises HTML, les URL et les adresses e-mail de notre jeu de données. Ces éléments étaient fréquents dans notre base de données en raison d'erreurs de récupération de données lors de la collecte de commentaires sur Wikipedia.

\end{document}
