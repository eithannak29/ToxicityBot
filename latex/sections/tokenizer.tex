\chapter{Tokenizer}

\section{Pré-traitement du jeu de données}

Le pré-traitement des données est une étape essentielle dans le processus d'analyse de texte. Il vise à nettoyer, transformer et préparer les données brutes afin de les rendre exploitables pour l'entraînement des modèles. Dans cette section, nous appliquerons différentes techniques de pré-traitement sur notre jeu de données afin de le rendre apte à être utilisé dans nos modèles prédictifs.

\subsection{Tokenisation à base d’expressions régulières (RegexTokenizer)}

La tokenisation est le processus de division du texte en unités plus petites appelées "tokens". La tokenisation à base d’expressions régulières utilise des règles basées sur des motifs d'expressions régulières pour diviser le texte en tokens.

\subsubsection*{Description de la méthode de tokenisation à base d'expressions régulières}

La tokenisation à base d'expressions régulières consiste à diviser le texte en tokens en utilisant des règles définies par des motifs d'expressions régulières. Ces motifs permettent de reconnaître les limites entre les mots, les ponctuations, etc.

\subsubsection*{Implémentation de la tokenisation à l'aide de la classe RegexTokenizer}

Nous commençons par importer la classe RegexTokenizer et appliquer la tokenisation sur notre jeu de données.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Avant tokenisation} & \textbf{Après tokenisation} \\ \hline
``Hello, World!'' & ['Hello', ',', 'World', '!'] \\ \hline
``I love NLTK'' & ['I', 'love', 'NLTK'] \\ \hline
\end{tabular}
\caption{Exemple de tokenisation à base d'expressions régulières}
\end{table}

\subsection{Tokenisation byte-pair encoding (TikToken)}

Le byte-pair encoding (BPE) est une méthode de tokenisation qui découpe le texte en sous-unités de texte appelées "tokens" en utilisant un algorithme de compression de données.

\subsubsection*{Description de la méthode de tokenisation byte-pair encoding}

La tokenisation BPE découpe le texte en sous-unités de texte de taille variable, appelées "tokens", en utilisant un algorithme de compression de données.

\subsubsection*{Implémentation de la tokenisation à l'aide de la bibliothèque TikToken}

Nous appliquons la tokenisation BPE sur notre jeu de données en utilisant la bibliothèque TikToken.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Avant tokenisation} & \textbf{Après tokenisation} \\ \hline
``Hello, World!'' & ['Hello', ',', 'World', '!'] \\ \hline
``I love TikToken'' & ['I', 'love', 'Ti', 'k', 'To', 'ken'] \\ \hline
\end{tabular}
\caption{Exemple de tokenisation BPE}
\end{table}

\subsection{Comparaison avec d'autres tokenizers}

Nous avons également comparé la performance de notre tokenizer avec d'autres options disponibles dans la bibliothèque MinBPE.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Tokenizer} & \textbf{Temps d'entrainement (minutes)} \\ \hline
RegexTokenizer & ??? \\ \hline
TikToken & 0 (déja entrainé) \\ \hline
BasicTokenizer & 69 \\ \hline
\end{tabular}
\caption{Comparaison des temps d'entrainement des tokenizers}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Tokenizer} & \textbf{Temps d'éxecution (seconde)} \\ \hline
RegexTokenizer & ??? \\ \hline
TikToken & ??? \\ \hline
BasicTokenizer & ??? \\ \hline
\end{tabular}
\caption{Comparaison des temps d'éxecution des tokenizers}
\end{table}


Nous constatons que le RegexTokenizer et le TikToken sont significativement plus rapides que le BasicTokenizer. TikToken est le plus rapide parmi tous les tokenizers testés.

\subsection{Méthodes de normalisation du texte}

La normalisation du texte est une étape cruciale du pré-traitement des données textuelles. Elle vise à uniformiser le texte en le mettant en minuscules, en supprimant les stop words et en lemmatisant les mots.

\subsubsection*{Suppression des stop words}

Les stop words sont des mots courants qui n'apportent pas beaucoup de valeur sémantique au texte. Nous les supprimerons de notre jeu de données.

\subsubsection*{Lemmatisation}

La lemmatisation consiste à réduire les mots fléchis ou dérivés à leur forme de base ou racine. Cela permet de normaliser le texte et de réduire la dimensionnalité de l'espace des features.

\subsubsection*{Mise en minuscules}

La mise en minuscules permet d'uniformiser le texte en convertissant toutes les lettres en minuscules. Cela permet d'éviter les doublons dus à la casse.

\subsubsection*{Identification des verbes et des noms}

Nous avons modifié la méthode de lemmatisation pour identifier les verbes et les noms dans une phrase. Parfois, les verbes étaient considérés comme des noms et ne pouvaient pas être lemmatisés avec le lemmatiseur par défaut.

\subsection{Pré-traitement supplémentaire du jeu de données}

En plus des techniques de pré-traitement déjà mentionnées, nous avons également effectué les actions suivantes pour rendre notre jeu de données plus adapté à l'entraînement de nos modèles :

\subsubsection*{Conservation des caractères spéciaux}

Nous avons choisi de ne pas supprimer les caractères spéciaux du texte. En effet, la ponctuation, par exemple, peut être importante pour détecter la toxicité dans le langage (par exemple, "!!!!").

\subsubsection*{Conservation des symboles '=' et des chiffres}

Nous avons décidé de conserver les symboles '=' et les chiffres dans le texte, car ils peuvent être utilisés dans des expressions symboliques (par exemple, "8==>" pour représenter un penis).

\subsubsection*{Suppression des balises HTML, des URL et des adresses e-mail}

Nous avons supprimé les balises HTML, les URL et les adresses e-mail de notre jeu de données. Ces éléments étaient fréquents dans notre base de données en raison d'erreurs de récupération de données lors de la collecte de commentaires sur Wikipedia.

\end{document}
