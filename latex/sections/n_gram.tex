\chapter{N-gram}

\section{Pré-traitement des données}

Nous avons d'abord effectué un pré-traitement des données en utilisant différentes techniques :

\subsection{Tokenisation des commentaires}

Nous avons utilisé la tokenisation pour diviser les commentaires en tokens individuels.

\subsubsection*{Tokenisation à l'aide de nôtre API \texttt{tokenize\_apy}}

La tokenisation des commentaires a été réalisée en utilisant l'API \texttt{tokenize\_apy}.

\subsubsection*{Équilibrage du jeu de données}

Nous avons équilibré le jeu de données en échantillonnant un nombre égal de commentaires toxiques et non toxiques.

\section{Analyse exploratoire des données}

Voici les statistiques descriptives sur le jeu de données équilibré :

\subsection{Statistiques descriptives}

\begin{itemize}
    \item Nombre total de documents : 223 549
    \item Nombre total de tokens : Variable selon les n-grams
    \item Nombre de classes à prédire : Non applicable pour les n-grams
    \item Les n-grams les plus fréquents incluent "nigger nigger", "on my", "be blocked", etc.
\end{itemize}

\section{Entraînement du modèle}

Nous avons entraîné le modèle N-gram en suivant les étapes suivantes :

\subsection{Préparation des données}

\begin{itemize}
    \item Utilisation de \texttt{CountVectorizer} pour créer des bigrams et trigrams.
    \item Entraînement du modèle en utilisant les fréquences des n-grams.
\end{itemize}

\subsection{Évaluation des performances du modèle}

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    \textbf{N-gram} & \textbf{Précision} & \textbf{Rappel} & \textbf{F1-score} \\ \hline
    Bigram & \textbf{0.75} & \textbf{0.75} & \textbf{0.75} \\ \hline
    Trigram & 0.72 & 0.72 & 0.72 \\ \hline
    4-gram & 0.70 & 0.70 & 0.70 \\ \hline
    \end{tabular}
    \caption{Performances du modèle N-gram avec différentes configurations}
\end{table}

\subsection{Validation croisée}

Nous avons également effectué une validation croisée en utilisant la métrique \texttt{accuracy}.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Métrique} & \textbf{Score} \\ \hline
    accuracy (moyenne) & 0.723 \\ \hline
    accuracy (écart type) & 0.018 \\ \hline
    \end{tabular}
    \caption{Résultats de la validation croisée}
\end{table}

\section{Analyse des résultats}

\subsection{Fréquence des n-grams}

Nous avons calculé la fréquence des bigrams et des trigrams les plus courants dans le jeu de données.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Bigram} & \textbf{Fréquence} \\ \hline
    nigger nigger & 3411 \\ \hline
    on my & 3067 \\ \hline
    be blocked & 2761 \\ \hline
    fuck you & 2681 \\ \hline
    \end{tabular}
    \caption{Fréquence des bigrams les plus courants}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Trigram} & \textbf{Fréquence} \\ \hline
    once upon a & 26 \\ \hline
    on my talk & 20 \\ \hline
    be blocked from & 15 \\ \hline
    \end{tabular}
    \caption{Fréquence des trigrams les plus courants}
\end{table}

\subsection{Prédiction}

Nous avons utilisé les modèles bigram et trigram pour prédire le mot suivant dans une phrase donnée.

\begin{itemize}
    \item Exemple de prédiction bigram : "Do you like playing" $\rightarrow$ "the"
    \item Exemple de prédiction trigram : "once upon a" $\rightarrow$ "time"
\end{itemize}

\subsection{Matrice de confusion}

La matrice de confusion montre les performances du modèle pour les prédictions correctes et incorrectes.

\begin{figure}[h]
    \centering
    \includegraphics[width=.49\linewidth]{figures/matrix-confusion-ngram.png}
    \caption{Matrice de confusion du modèle N-gram}
\end{figure}

\subsection{Fonction de prédiction}

Nous avons également créé une fonction \texttt{complete\_the\_phrase} qui prend en entrée une phrase de départ et prédit la suite.

\begin{itemize}
    \item Exemple d'utilisation : "once upon" $\rightarrow$ "a time when i was just a few days ago i m not"
\end{itemize}

\section{Distribution de la fréquence des n-grams}

\begin{figure}[h]
    \centering
    \includegraphics[width=.47\linewidth]{figures/distribution-freq-ngram.png}
    \caption{Distribution de la fréquence des n-grams dans les commentaires}
\end{figure}

\end{document}
