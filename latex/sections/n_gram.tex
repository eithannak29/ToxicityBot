\chapter{N-gram}

\section{Prétraitement des données}

Le prétraitement des données a été initié en utilisant diverses techniques visant à améliorer la qualité des entrées pour le modèle \textit{N-gram}.

\subsubsection{Tokenisation des commentaires}

La tokenisation divise les commentaires en unités de base appelées \textit{tokens}, facilitant l'analyse et la modélisation des données textuelles.

\subsubsection*{Tokenisation à l'aide de \textit{API} \textit{tokenize\_api}}

L'API \textit{tokenize\_api} a été employée pour la tokenisation des commentaires. Cette \textit{API} permet la transformation des phrases en une séquence de mots individuels ou \textit{tokens}, essentielle à la création des \textit{n-grams}.

\subsubsection*{Équilibrage du jeu de données}

Afin d'éviter les biais dans le modèle, le jeu de données a été équilibré en échantillonnant un nombre égal de commentaires toxiques et non toxiques. Cette étape est cruciale pour garantir une apprentissage équitable des deux types de données par le modèle.

\subsubsection{Analyse exploratoire des données}

Avant de procéder à l'entraînement du modèle, une analyse exploratoire a été réalisée afin de comprendre les caractéristiques du jeu de données.

\subsubsection{Statistiques descriptives}

Voici les statistiques descriptives sur le jeu de données équilibré :

\begin{itemize}
    \item Nombre total de documents : 223 549
    \item Nombre total de \textit{tokens} : Variable selon les \textit{n-grams}
    \item Les \textit{n-grams} les plus fréquents incluent \textit{"nigger nigger", "on my", "be blocked", etc.}
\end{itemize}

\section{Entraînement du modèle}

Le modèle \textit{N-gram} a été entraîné en suivant une série d'étapes méthodiques.

\subsubsection{Préparation des données}

\begin{itemize}
    \item Utilisation de \textit{CountVectorizer} pour créer des \textit{bigrams} et \textit{trigrams}
    \item Entraînement du modèle en utilisant les fréquences des \textit{n-grams} extraits des commentaires.
\end{itemize}

\subsubsection{Création du modèle de bigrams et trigrams}

\textit{CountVectorizer} a été utilisé pour créer des modèles de \textit{bigrams} (séquences de deux mots) et \textit{trigrams} (séquences de trois mots) à partir des phrases.

\subsubsection{Fréquence des \textit{n-grams}}

La fréquence des \textit{n-grams} les plus courants dans le jeu de données a été calculée afin d'obtenir une meilleure compréhension de leur distribution.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Bigram} & \textbf{Fréquence} \\ \hline
    nigger nigger & 3411 \\ \hline
    on my & 3067 \\ \hline
    be blocked & 2761 \\ \hline
    fuck you & 2681 \\ \hline
    \end{tabular}
    \caption{Fréquence des \textit{bigrams} les plus courants}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Trigram} & \textbf{Fréquence} \\ \hline
    once upon a & 26 \\ \hline
    on my talk & 20 \\ \hline
    be blocked from & 15 \\ \hline
    \end{tabular}
    \caption{Fréquence des \textit{trigrams} les plus courants}
\end{table}

\subsubsection{Fonction de prédiction}

Des fonctions de prédiction ont été définies pour compléter les phrases basées sur les \textit{n-grams} les plus fréquents.

\subsubsection*{Prédiction avec les \textit{bigrams}}

\begin{itemize}
    \item Exemple de prédiction \textit{bigram} : "Do you like playing" $\rightarrow$ "the"
\end{itemize}

\subsubsection*{Prédiction avec les \textit{trigrams}}

\begin{itemize}
    \item Exemple de prédiction \textit{trigram} : "once upon a" $\rightarrow$ "time"
\end{itemize}

\subsubsection*{Complétion de phrase}

Une fonction \textit{complete\_the\_phrase} a également été créée, prenant en entrée une phrase de départ et prédisant la suite.

\begin{itemize}
    \item Exemple d'utilisation : "once upon" $\rightarrow$ "a time when I was just a few days ago I’m not"
\end{itemize}

\section{Évaluation des performances du modèle}

Afin d'évaluer les performances du modèle, l'\textit{accuracy} a été calculée. Le modèle a été testé sur une cinquantaine de phrases concaténées pour obtenir ces résultats.
\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{N-gram} & \textbf{Accuracy} \\ \hline
    \textit{Bigram} & 0.0925   \\ \hline
    \textit{Trigram} & 0.0925  \\ \hline
    % 4-gram & 0.0 \\ \hline
    \end{tabular}
    \caption{Performances du modèle \textit{N-gram} avec différentes configurations (sur 50 phrases)}
\end{table}

\section{Analyse des résultats}

\subsubsection{Interprétation des fréquences des \textit{n-grams}}

Les \textit{n-grams} les plus fréquents identifiés révèlent des schémas de langage courants et des expressions fréquentes dans le corpus. Ces informations sont cruciales pour améliorer la compréhension et la génération de texte.

\subsubsection{Prédiction et complétion de phrases}

Les modèles de \textit{bigrams} et \textit{trigrams} ont été utilisés pour prédire le mot suivant dans une phrase donnée, et pour compléter des phrases entières. Cela démontre l'efficacité des \textit{n-grams} pour la génération de texte basée sur des séquences de mots précédents.

% \subsubsection*{Exemples de prédiction et complétion}

% \begin{itemize}
%     \item Exemple de prédiction bigram : "Do you like playing" $\rightarrow$ "the"
%     \item Exemple de prédiction trigram : "once upon a" $\rightarrow$ "time"
%     \item Complétion de phrase : "once upon" $\rightarrow$ "a time when I was just a few days ago I’m not"
% \end{itemize}

\subsubsection{Conclusion}

Le modèle \textit{N-gram}, bien que simple, offre une capacité intéressante pour prédire et générer du texte basé sur les séquences de mots précédents. Cependant, il est limité par la taille du contexte qu'il peut utiliser (les {n-grams}). Des modèles plus avancés, comme les modèles de langage basés sur les réseaux de neurones, peuvent utiliser un contexte plus large et capturer des relations plus complexes dans les données textuelles.