\chapter{N-gram}

\section{Pré-traitement des données}

Nous avons d'abord effectué un pré-traitement des données en utilisant différentes techniques pour améliorer la qualité des entrées pour le modèle N-gram.

\subsection{Tokenisation des commentaires}

La tokenisation divise les commentaires en unités de base appelées tokens, facilitant l'analyse et la modélisation des données textuelles.

\subsubsection*{Tokenisation à l'aide de notre API \texttt{tokenize\_apy}}

Nous avons utilisé l'API \texttt{tokenize\_apy} pour effectuer la tokenisation des commentaires. Cette API nous permet de transformer les phrases en une séquence de mots individuels ou tokens, essentielle pour la création des n-grams.

\subsubsection*{Équilibrage du jeu de données}

Pour éviter les biais dans notre modèle, nous avons équilibré le jeu de données en échantillonnant un nombre égal de commentaires toxiques et non toxiques. Cette étape est cruciale pour s'assurer que le modèle apprend de manière équitable des deux types de données.

\section{Analyse exploratoire des données}

Avant de procéder à l'entraînement du modèle, nous avons effectué une analyse exploratoire pour comprendre les caractéristiques de notre jeu de données.

\subsection{Statistiques descriptives}

Voici les statistiques descriptives sur le jeu de données équilibré :

\begin{itemize}
    \item Nombre total de documents : 223 549
    \item Nombre total de tokens : Variable selon les n-grams
    \item Les n-grams les plus fréquents incluent "nigger nigger", "on my", "be blocked", etc.
\end{itemize}

\section{Entraînement du modèle}

Nous avons entraîné notre modèle N-gram en suivant une série d'étapes méthodiques.

\subsection{Préparation des données}

\begin{itemize}
    \item Utilisation de \texttt{CountVectorizer} pour créer des bigrams, trigrams et 4-grams.
    \item Entraînement du modèle en utilisant les fréquences des n-grams extraits des commentaires.
\end{itemize}

\subsection{Création du modèle de bigrams et trigrams}

Nous avons utilisé `CountVectorizer` pour créer des modèles de bigrams (séquences de deux mots) et trigrams (séquences de trois mots) à partir des phrases. 

\subsection{Fréquence des n-grams}

Nous avons calculé la fréquence des n-grams les plus courants dans le jeu de données pour avoir une meilleure compréhension de leur distribution.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Bigram} & \textbf{Fréquence} \\ \hline
    nigger nigger & 3411 \\ \hline
    on my & 3067 \\ \hline
    be blocked & 2761 \\ \hline
    fuck you & 2681 \\ \hline
    \end{tabular}
    \caption{Fréquence des bigrams les plus courants}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Trigram} & \textbf{Fréquence} \\ \hline
    once upon a & 26 \\ \hline
    on my talk & 20 \\ \hline
    be blocked from & 15 \\ \hline
    \end{tabular}
    \caption{Fréquence des trigrams les plus courants}
\end{table}

\subsection{Fonction de prédiction}

Nous avons défini des fonctions de prédiction pour compléter les phrases basées sur les n-grams les plus fréquents.

\subsubsection*{Prédiction avec les bigrams}

\begin{itemize}
    \item Exemple de prédiction bigram : "Do you like playing" $\rightarrow$ "the"
\end{itemize}

\subsubsection*{Prédiction avec les trigrams}

\begin{itemize}
    \item Exemple de prédiction trigram : "once upon a" $\rightarrow$ "time"
\end{itemize}

\subsubsection*{Complétion de phrase}

Nous avons également créé une fonction \texttt{complete\_the\_phrase} qui prend en entrée une phrase de départ et prédit la suite.

\begin{itemize}
    \item Exemple d'utilisation : "once upon" $\rightarrow$ "a time when I was just a few days ago I’m not"
\end{itemize}

\section{Évaluation des performances du modèle}

Pour évaluer les performances de notre modèle, nous avons calculé plusieurs métriques de performance, incluant la précision, le rappel et le F1-score pour différentes configurations de n-grams.
On test sur les une cinquentaine de phrase le modèle configuré pour obtenir ces resultas.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{N-gram} & \textbf{Précision} & \textbf{Rappel} & \textbf{F1-score} \\ \hline
    Bigram & 0.0 & 0.0 & 0.0 \\ \hline
    Trigram & 0.0 & 0.0 & 0.0 \\ \hline
    4-gram & 0.0 & 0.0 & 0.0 \\ \hline
    \end{tabular}
    \caption{Performances du modèle N-gram avec différentes configurations}
\end{table}

\section{Analyse des résultats}

\subsection{Interprétation des fréquences des n-grams}

Les n-grams les plus fréquents identifiés révèlent des schémas de langage courants et des expressions fréquentes dans le corpus. Ces informations sont cruciales pour améliorer la compréhension et la génération de texte.

\subsection{Prédiction et complétion de phrases}

Les modèles de bigrams et trigrams ont été utilisés pour prédire le mot suivant dans une phrase donnée, et pour compléter des phrases entières. Cela démontre l'efficacité des n-grams pour la génération de texte basée sur des séquences de mots précédents.

% \subsubsection*{Exemples de prédiction et complétion}

% \begin{itemize}
%     \item Exemple de prédiction bigram : "Do you like playing" $\rightarrow$ "the"
%     \item Exemple de prédiction trigram : "once upon a" $\rightarrow$ "time"
%     \item Complétion de phrase : "once upon" $\rightarrow$ "a time when I was just a few days ago I’m not"
% \end{itemize}

\section{Conclusion}

Le modèle N-gram, bien que simple, offre une capacité intéressante pour prédire et générer du texte basé sur les séquences de mots précédents. Cependant, il est limité par la taille du contexte qu'il peut utiliser (les n-grams). Des modèles plus avancés, comme les modèles de langage basés sur les réseaux de neurones, peuvent utiliser un contexte plus large et capturer des relations plus complexes dans les données textuelles.